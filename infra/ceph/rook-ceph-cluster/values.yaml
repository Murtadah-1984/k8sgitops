# CephCluster configuration
# Note: RGW (Rados Gateway) is disabled - no CephObjectStore resource exists
# Note: NFS is disabled - no CephNFS resource exists
cephCluster:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.0
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 1
    # Disable expensive MGR modules to save ~1.5GB RAM
    # These modules will be disabled via CephCluster spec
    modules:
      - name: dashboard
        enabled: false
      - name: prometheus
        enabled: false
      - name: nfs
        enabled: false
      - name: restful
        enabled: false
  network:
    provider: host
    selectors:
      public: "storage"
      cluster: "storage"
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: k8s-ceph-02
        devices:
          - name: /dev/nvme0n1
      - name: k8s-ceph-03
        devices:
          - name: /dev/nvme0n1
    config:
      osdsPerDevice: "1"
      osd_pool_default_size: "2"
  resources:
    mgr:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    mon:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    osd:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
  placement:
    all:
      tolerations: []
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values:
              - enabled
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - enabled
blockPools:
  - name: replicapool
    spec:
      replicated:
        size: 2
      parameters:
        compression_mode: aggressive
        compression_algorithm: zstd
        readahead: "128K"
        recovery_sleep: "0.1"
    storageClass:
      name: rook-ceph-block
      isDefault: true
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/fstype: xfs

# CephFS filesystems
cephFilesystems:
  - name: myfs
    spec:
      metadataPool:
        replicated:
          size: 2
      dataPools:
        - name: data0
          replicated:
            size: 2
      preserveFilesystemOnDelete: true
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            memory: 1Gi
    storageClass:
      name: rook-cephfs
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        clusterID: rook-ceph
        fsName: myfs
        pool: data0
        csi.storage.k8s.io/fstype: ext4

toolbox:
  enabled: true

